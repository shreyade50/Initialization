# Initialization
A well-chosen initialization can:  Speed up the convergence of gradient descent Increase the odds of gradient descent converging to a lower training (and generalization) error
